{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNYtXYN65rTTZ3ztI9CsYWE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Caiozotex/mc322---TurmaX/blob/master/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chatbot baseado em IA para responder dúvidas sobre o vestibular da Unicamp 2024**\n"
      ],
      "metadata": {
        "id": "6kqtPOo8mRgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui instalaremos o framework Langchain e as bibliotecas TransformersFaiss,gradio,pyngrok e nltk.\n",
        "\n",
        "**Langchain** é um framework de código aberto que facilita o treinamento de modelos de linguagem com base em cadeias longas de texto. Ele fornece uma infraestrutura eficiente para lidar com dados de texto extensos e complexos.\n",
        "\n",
        "**Transformers**: Uma biblioteca da Hugging Face que oferece uma ampla variedade de modelos de linguagem pré-treinados, incluindo BERT, GPT, RoBERTa, entre outros. Esses modelos podem ser usados para tarefas como classificação de texto, geração de texto, tradução automática e muito mais.\n",
        "\n",
        "**Faiss**: Uma biblioteca de indexação e busca eficiente para vetores grandes, frequentemente usada em aplicativos de recuperação de informações e aprendizado de máquina.\n",
        "\n",
        "**Gradio**: Uma biblioteca que simplifica a criação de interfaces de usuário para modelos de aprendizado de máquina e ferramentas de processamento de linguagem natural. Ela permite que os usuários interajam com modelos através de uma interface amigável na web.\n",
        "\n",
        "**Pyngrok**: Uma ferramenta que expõe localmente um servidor Flask, Django ou qualquer aplicativo da web para a internet usando túneis seguros. Isso é útil para compartilhar rapidamente aplicativos em desenvolvimento ou modelos de aprendizado de máquina com outras pessoas.\n",
        "\n",
        "**NLTK** (Natural Language Toolkit): Uma biblioteca Python amplamente utilizada para processamento de linguagem natural. Ela oferece suporte para uma ampla gama de tarefas, como tokenização, stemming, lematização, análise sintática, entre outras."
      ],
      "metadata": {
        "id": "IJdx9CYunff-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain transformers faiss-cpu pyngrok\n",
        "!pip install sentence-transformers pdfplumber\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "jcAp9_HTpfWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta etapa usaremos a biblioteca pdfplumber para podermos extrair as informações da Resolução GR-031/2023, de 13/07/2023. Salvei o arquivo como pdf e coloquei na pasta sample_data. Neste processo também transformei as tabelas existentes em texto e tirei as imagens do arquivo"
      ],
      "metadata": {
        "id": "J6K2TKhVoLYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "def extract_text_with_layout(pdf_path):\n",
        "    all_text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            # Extract tables\n",
        "            tables = page.extract_tables()\n",
        "            for table in tables:\n",
        "                # Convert table to string or handle as needed\n",
        "                table_text = \"\\n\".join([\"\\t\".join(row) for row in table])\n",
        "                page_text += \"\\n\" + table_text\n",
        "            all_text += page_text + \"\\n\"\n",
        "    return all_text\n",
        "\n",
        "resolution_text = extract_text_with_layout(\"/content/sample_data/Procuradoria Geral - Normas.pdf\")"
      ],
      "metadata": {
        "id": "PYh07_4kc3i8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos dividir o texto em pedaços menores, chamados de \"chunks\", para processamento; chunk_size define o tamanho de cada chunk (pedaço) em termos de número de sentenças e\n",
        "overlap define o tamanho da sobreposição entre os chunks. Neste caso, 2 sentenças de cada chunk serão compartilhadas com o chunk anterior. Isso é útil para processar grandes volumes de texto em partes menores, facilitando a análise e o processamento.\n",
        "\n",
        "Esta parte foi dificíl pois apesar do ChatGPT indicar um método automático para achar o chunk_size e o overlap, tive que ajustar manualmente para encontrar resultados melhores."
      ],
      "metadata": {
        "id": "KjPFOCKMpUB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text into smaller chunks if necessary\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(resolution_text)\n",
        "chunk_size = 4 # Define the size of each chunk\n",
        "overlap = 2\n",
        "\n",
        "chunks = []\n",
        "for i in range(0, len(sentences), chunk_size - overlap):\n",
        "    chunk = sentences[i:i + chunk_size]\n",
        "    chunks.append(' '.join(chunk))"
      ],
      "metadata": {
        "id": "GCF6oa4l2Nsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui implementamos o chatbot de fato. Usando o modelo BART (Bidirectional and Auto-Regressive Transformers) para processar cada chunk de texto (documentos) e adicionar ao índice FAISS, em  que cada chunk é codificado em embeddings de sentença pelo embedder e, em seguida, adicionado ao índice FAISS. Por fim geramos uma resposta para uma dada consulta, buscando os contextos relevantes usando a função search e, em seguida, gerar uma resposta. No final fazemos um teste simples para testar nosso modelo.\n",
        "\n",
        "A metodologia foi:   \n",
        "                            \n",
        "Inicialização dos Modelos e Ferramentas:Para representar o texto e calcular a similaridade entre as sentenças, utilizamos o SentenceTransformer para obter embeddings de sentenças e o faiss para construir e consultar um índice de similaridade eficiente.\n",
        "Para gerar as respostas do chatbot, utilizamos o modelo BartForConditionalGeneration da biblioteca Transformers, que foi treinado para tarefas de geração de texto condicional.\n",
        "Pré-processamento dos Documentos:\n",
        "\n",
        "O código processa os documentos (ou chunks de texto) divididos anteriormente e os converte em embeddings vetoriais usando o embedder. Esses embeddings são então adicionados ao índice FAISS para uma busca eficiente durante a geração de respostas.\n",
        "Definição de Funções de Busca e Geração de Resposta:\n",
        "\n",
        "A função search(query, index, embedder, top_k) é responsável por receber uma consulta de entrada, encontrar os documentos mais relevantes no índice FAISS e retornar os contextos associados a esses documentos.\n",
        "A função generate_response(query, index, embedder, model, tokenizer, top_k) usa os contextos relevantes obtidos pela função de busca para gerar uma resposta para a consulta. Ela integra o modelo BART para gerar respostas de texto condicionais com base nas consultas e nos contextos.\n",
        "Teste do Chatbot:\n",
        "\n",
        "O código final testa o sistema de resposta do chatbot, fornecendo uma consulta de teste e imprimindo a resposta gerada."
      ],
      "metadata": {
        "id": "N7bsVcS7qX_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import torch\n",
        "\n",
        "# Initialize the sentence transformer for embeddings\n",
        "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Initialize FAISS index\n",
        "dimension = 384  # Dimension of embeddings from 'paraphrase-MiniLM-L6-v2'\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Initialize the tokenizer and model from Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
        "\n",
        "# Preprocess documents and add to FAISS index\n",
        "documents = []\n",
        "for chunk_id, chunk in enumerate(chunks):\n",
        "    chunk_documents = {\"text\": chunk, \"id\": chunk_id}\n",
        "    documents.append(chunk_documents)\n",
        "    embeddings = embedder.encode([chunk])\n",
        "    index.add(embeddings.astype('float32'))\n",
        "\n",
        "# Define functions to search and generate responses\n",
        "def search(query, index, embedder, top_k=5):\n",
        "    query_embedding = embedder.encode([query])\n",
        "    _, indices = index.search(query_embedding.astype('float32'), k=top_k)\n",
        "    return [documents[idx][\"text\"] for idx in indices[0]]\n",
        "\n",
        "def generate_response(query, index, embedder, model, tokenizer, top_k=5):\n",
        "    contexts = search(query, index, embedder, top_k)\n",
        "    context = \" \".join(contexts)\n",
        "    inputs = tokenizer(query + \" \" + context, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=150, no_repeat_ngram_size=2, num_beams=5, early_stopping=True)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test the chatbot response\n",
        "query = \"Quantas vagas regulares são oferecidas para ingresso\"\n",
        "response = generate_response(query, index, embedder, model, tokenizer)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "eJO5lQmxqz26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para avaliar nosso modelo de forma automática avaliamos a precisão do modelo, que é a razão entre o número de respostas pela numero total de respostas"
      ],
      "metadata": {
        "id": "aCvgiSHgtVhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample test queries and expected responses\n",
        "test_cases = [\n",
        "    {\"query\": \"Quantas vagas regulares são oferecidas para ingresso\", \"expected\": \"Para o ano de 2024 são oferecidas 3340 vagas regulares para ingresso nos Cursos de Graduação da Unicamp\"},\n",
        "    {\"query\": \"Qual o período de inscrições\", \"expected\": \"O período para inscrições no VU 2024 será de 31 de julho a 31 de agosto de 2023.\"},\n",
        "    # Add more test cases\n",
        "]\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_chatbot(test_cases, index, model, tokenizer):\n",
        "    correct_responses = 0\n",
        "    total_responses = len(test_cases)\n",
        "\n",
        "    for case in test_cases:\n",
        "        query = case[\"query\"]\n",
        "        expected = case[\"expected\"]\n",
        "        response = generate_response(query, index, embedder, model, tokenizer)\n",
        "\n",
        "        # Simple accuracy measure: check if the response contains the expected answer\n",
        "        if expected.lower() in response.lower():\n",
        "            correct_responses += 1\n",
        "\n",
        "    accuracy = correct_responses / total_responses\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_chatbot(test_cases, index, model, tokenizer)"
      ],
      "metadata": {
        "id": "P_Mg91bKsb2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui criamos um arquivo app.py para podermos rodar atravé de uma página web"
      ],
      "metadata": {
        "id": "RSzbuO-et8wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import gradio as gr\n",
        "import pdfplumber\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import torch\n",
        "\n",
        "def extract_text_with_layout(pdf_path):\n",
        "    all_text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            # Extract tables\n",
        "            tables = page.extract_tables()\n",
        "            for table in tables:\n",
        "                # Convert table to string or handle as needed\n",
        "                table_text = \"\\n\".join([\"\\t\".join(row) for row in table])\n",
        "                page_text += \"\\n\" + table_text\n",
        "            all_text += page_text + \"\\n\"\n",
        "    return all_text\n",
        "\n",
        "resolution_text = extract_text_with_layout(\"/content/sample_data/Procuradoria Geral - Normas.pdf\")\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(resolution_text)\n",
        "chunk_size = 4 # Define the size of each chunk\n",
        "overlap = 2\n",
        "\n",
        "chunks = []\n",
        "for i in range(0, len(sentences), chunk_size - overlap):\n",
        "    chunk = sentences[i:i + chunk_size]\n",
        "    chunks.append(' '.join(chunk))\n",
        "\n",
        "# Initialize the sentence transformer for embeddings\n",
        "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Initialize FAISS index\n",
        "dimension = 384  # Dimension of embeddings from 'paraphrase-MiniLM-L6-v2'\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Initialize the tokenizer and model from Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
        "\n",
        "# Preprocess documents and add to FAISS index\n",
        "documents = []\n",
        "for chunk_id, chunk in enumerate(chunks):\n",
        "    chunk_documents = {\"text\": chunk, \"id\": chunk_id}\n",
        "    documents.append(chunk_documents)\n",
        "    embeddings = embedder.encode([chunk])\n",
        "    index.add(embeddings.astype('float32'))\n",
        "\n",
        "# Define functions to search and generate responses\n",
        "def search(query, index, embedder, top_k=3):\n",
        "    query_embedding = embedder.encode([query])\n",
        "    _, indices = index.search(query_embedding.astype('float32'), k=top_k)\n",
        "    return [documents[idx][\"text\"] for idx in indices[0]]\n",
        "\n",
        "def generate_response(query, index, embedder, model, tokenizer, top_k=3):\n",
        "    contexts = search(query, index, embedder, top_k)\n",
        "    context = \" \".join(contexts)\n",
        "    inputs = tokenizer(query + \" \" + context, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=150, num_beams=5, early_stopping=True)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Gradio interface\n",
        "def chatbot_interface(query):\n",
        "    return generate_response(query, index, embedder, model, tokenizer)\n",
        "\n",
        "gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"Chatbot do vestibular Unicamp 2024\").launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-9H_4B6-p5L",
        "outputId": "8392ea45-fcee-424e-98ee-3ca84629d6d3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui rodamos o código na página web. Você pode usar uma conexão local ou usar uma URL pública que ficará até 72 horas no ar"
      ],
      "metadata": {
        "id": "kqmvUJZTuXiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "id": "w3BYk9h4O8IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " O desenvolvimento,concepção,codificação, avaliação e escrita de relatório tiveram ajuda do ChatGPT. Apesar de um resultado final satisfatório se eu tivesse um pouco mais tempo para o prazo final acho que poderia melhorar em outros aspectos como a resposta dada que tende a ser longa demais. Poderiamos aproveitar a experiencia desse chatbot e tentar construir um especificamente para responder perguntas referente ás informações presentes no site da DAC, visto que muitos alunos têm dificuldade com o sistema.\n",
        "\n"
      ],
      "metadata": {
        "id": "T7yt6FHozWMi"
      }
    }
  ]
}